# üåê GeoReddit_dev üåê

Development folder for spatio-temporal Reddit corpus

## üî¢ Analyses steps
1. Overview of scripts to create GeoLoctated corpus.
- **c_00:** It helps download the original Reddit dataset and rename the monthly .zst files by adding an o_ prefix for easier organization.
- **c_01:** It enables tracking how many posts each user made in state-subreddits every month, supporting later calculation of ratio1 and ratio2 and identifying users who can be geolocated to U.S. states, as part of the input for c_04.
- **c_02:** It is used to extract the part of the original Reddit dataset containing users who can be geolocated to U.S. states, creating the US State GeoReddit dataset.
- **c_03:** It is used to count the total number of posts made by geolocated users each month, as part of the input for c_04.
- **c_04:** To address the difficulty of determining a user's location when they post in multiple state-subreddits, it computes ratio1 and ratio2 metrics over a specified period to decide on how to locate a user to a unique state. Ratio1 is the ratio of the number of months a user was active in their top state to the total number of months they were active in any state. Ratio2 is the ratio of the number of posts a user made in their top state to the total number of posts they made in any state.
- **c_05:** Each plot shows how the cumulative proportion of users or posts varies when applying different thresholds to ratio1 or ratio2. According to these four figures, we chose ratio2 > 1 because this allows us to retain as many as 84% of posts and 95% of users in the GeoReddit dataset.
- **c_06:** After applying the ratio2 > 1 threshold in c_05, this script enables us to visualize the distribution of users and posts under this criterion and to calculate the number and ratio of users and posts retained or discarded.
- **c_07:** This script generates three choropleth maps to examine whether the geographic distributions of U.S. population, GeoReddit user counts, and GeoReddit post counts under this geolocation method show similarities or differences over a specified time period.
- **c_08:** It visualizes on a map approximately how many GeoReddit users there are per 1,000 residents in each state. It also calculates that the median and mean under this method are 21 users and 25 users per 1,000 residents,respectively.
- **c_09:** It generates summary statistics about Reddit user comment activity over a specified period, to help you compare: All Reddit users (from the raw dataset), Geo-located users (users assigned to U.S. states), and High-confidence geo-located users (those exceeding a specified ratio threshold, e.g., ratio2 > 1).
- **c_10:** This script checks how stable our method is for assigning Reddit users to U.S. states. It compares the state assigned to each user in a given year with their assignment in the full-period baseline. The script then reports how many users match, how many differ, and how much of the total population each year‚Äôs data can cover. This makes it easy to see whether the assignment method is reliable and whether coverage improves as Reddit grows over time.
  
2. Overview of scripts to analyse GeoLoctated corpus
- **a_00:** It builds a smart dictionary that helps automatically detect keywords or phrases in large amounts of text, even if they contain minor typos or inconsistent punctuation. To do this, it converts each keyword into a flexible search pattern that tolerates small spelling mistakes‚Äîsuch as missing, added, or changed letters‚Äîand allows for variations in spacing and punctuation. This logic ensures short or ambiguous terms are strictly matched to avoid false positives, while allowing typo-tolerant matching for longer, distinctive terms to improve recall.
- **a_01a:** It processes monthly GeoReddit datasets to identify and extract content related to a specific topic (e.g., ‚ÄúAI‚Äù). It employs a dictionary of regular expression patterns (from a_00) to capture both exact keywords and frequent misspellings, thereby enhancing coverage and recall. Posts are filtered to include only those users geolocated to U.S. states with high confidence (ratio2 > 1). The outputs comprise detailed records of all matched posts, including metadata such as username, subreddit, state affiliation, and counts of keyword occurrences. Also, It produces comprehensive summary tables quantifying the number and proportion of topic-related posts and users over time and across states, as well as the frequency of keywords. Additionally, it tracks runtime performance per month for improving running performance. This facilitates systematic analysis of temporal and geographic trends in topic discourse within the U.S. Reddit user base.
- **a_01b:** This script is a faster alternative to a_01a. It only checks whether a post contains any matched keyword, but does not count how many times each keyword appears. By default, it outputs topic-matched posts with basic metadata. If --full_output is enabled, it also includes the list of distinct matched keywords in each post and their count. The script saves time on large datasets by deferring keyword statistics until after LLM-based topic relevance confirmation, making it suitable for efficient filtering and downstream analysis.
- **a_01c:** It filters large Reddit datasets (like AllReddit or GeoReddit) by keywords, subreddits, or both, and can also map posts to U.S. states if location data is available. It processes compressed .zst files quickly using parallel computing, keeps only English content, and outputs monthly CSV files with the matched posts and detailed statistics for posts, authors, and states. It‚Äôs useful for tracking topic trends, analyzing specific communities, and preparing data for further analysis like sentiment or network studies.
- **a_03a&b&c:** It generates 12 gif maps that visualizes the proportion of discussion (user-level or post-level; ratio or absolute count) about a topic on Reddit by U.S state and over time (monthly, quarterly, or yearly aggregation). By normalizing the counts, it avoids biases caused by variations in total posting or user number. However, you can still get 6 maps related to absolute count of users and posts.
- **a_04:** it produces 3 colorful U.S. static maps that visualize the percentage of posts about the topic, the percentage of users discussing it, and the average number of keywords per post in each state. Also, you can get another 2 maps that visualize the post count and the user count about this tpoic in each state. This helps you quickly see where the topic is most popular and how widely it is discussed during the whole period.
- **a_05:** This script visualizes how a specific topic gains or loses popularity across U.S. states over time. It helps identify when and where public interest rises ‚Äî for example, during major events or breaking news ‚Äî using Reddit post or user data aggregated by month, quarter, or year.
- **a_06:** The core purpose of this script is to take your ‚Äútopic-related dictionary‚Äù as a list of keywords and, by scanning through the raw monthly data line by line, determine how many times each keyword appears in each month and how many times each pair of keywords co-occurs within the same post; then to aggregate across all months to compute for each keyword its total count, percentage of all posts, mean, median, number of zero-occurrence terms, and other metadata; and finally to build a co-occurrence network and calculate each keyword‚Äôs degree (the number of distinct co-occurrence partners). In other words, it ‚Äúquantifies‚Äù your dictionary into several CSV reports that reveal which terms are most popular, which are rarely used, and which occupy the most connected positions in the co-occurrence network.
- **a_07:** This script takes the outputs from a_06 as its input and, with a single command, generates a suite of charts to give you an intuitive understanding of your keywords:
‚Äì A long-tail distribution histogram showing the shape of total counts and node degrees;
‚Äì Top/Bottom N bar charts comparing the hottest and coldest terms;
‚Äì Word clouds that use size and color to visualize the relative weights of the top and bottom terms;
‚Äì Time-series line plots tracking the monthly trends of the top N terms;
‚Äì A four-quadrant co-occurrence scatterplot that places ‚Äúmention count‚Äù against ‚Äúco-occurrence degree‚Äù to quickly identify core terms, isolated hotspots, bridge-potential terms, and niche terms;
‚Äì A hierarchical clustering dendrogram built from the co-occurrence network to reveal naturally emerging thematic clusters.
- **a_08:** This script identifies emotional words in the post body based on based on NCRLex (National Research Council Emotion Lexicon) . Words are identified to be related to eight emotions: 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust' and two types of sentiment: 'positive', 'negative'. The script counts the number of occurences of words (count), the proportion of how often it appears in comparison to the other emotional words (prop), and the relative share of occurences in comparison to the total amount of words (freq).
- **a_09:** This script generates interactive dynamic choropleth maps (and their underlying .csv data) to visualize average sentiment (estimated by NRCLex) across US states over time (monthly, quarterly, or yearly aggregation). This script provides different cutoff values >10, >30, >50, >100 observations per state x period combination. Also it gives an overview excel of how many observations there are for each state x period combination.
- **a_10:** This script generates linegraphs of NRCL emotions and sentiment over time (monthly, quarterly, or yearly aggregation) as .png-files.

3. Overview of scripts for validation
- **v_01a&b:** To demonstrate the validity of this geolocation method, v_01a aggregates monthly data from 2005 to 2023 into yearly counts of unique GeoReddit users (ratio2 > 1) and total posts by state, as input for 28 plots in v_01b; v_01b generates scatter plots showing the correlations between U.S. state resident population census or estimates (from U.S.Census Bureau) and GeoReddit user/post counts over a specified period. And v_01b also produces 28 subplots, two for each year from 2010 to 2023. The correlation coefficients (2005-2023) are 0.90 for user counts and 0.90 for post counts, demonstrating the reliability of this geolocation method. 
- **v_02a&b:** The goal is to validate whether online participation in different subgroups (e.g., occupations, religions, racial/ethnic groups) reflects their actual state-level distributions in the United States.  v_02a takes geolocated Reddit activity and groups it into broader categories (called ‚Äútypes‚Äù) using a predefined mapping from subreddits to types. The result is a dataset that shows, for each user and time period (month or day level), how often they participated (post count) in a given type, providing the foundation for comparing Reddit activity to real-world population patterns. Using the result from v_02a, v_02b produces visual evidence to check whether Reddit-based geolocation patterns align with real-world data. By doing so, it produces tables summarizing correlations, scatterplots illustrating state-level alignment between Reddit-based and real-world data, and optional U.S. maps that directly compare actual distributions with Reddit-derived user and post distributions.

4. Overview of scripts for projects
- **e_v1_01:** This script generates a .png on the overall count of unique authors posting about Pepsi and their mean NRCL_freq_anger per month with marking the analysed event month of April 2017. Furthermore, the script generates the a .csv file for the respective unique author count and men NRCL_freq_anger data. In addition, the script computes the total amount of authors for the relevant event month, and the relative change in authors commenting, and the relative change in anger-related words overall and per state.
- **e_v1_02:** This script generates choropleth maps (and its corresponding data as .csv) to visualize the change in average sentiment (estimated by NRCLex anger) across US states between April 2017 and the reference period January 2014 - March 2017. Maps are created for NRCL_freq_anger or NRCL_prop_anger and the cutoffs by 30 or 50 observations.
- **e_v1_03:** This script generates correlations between sentiment change (i.e, change in anger) and various regional, state-level covariates, and visualizes them in a forest plot. In addition, this script generates a scatterplot for the correlation between the outcome variable and voting Republican in the 2016 presidential election. Plots vary by outcome variable, cutoff, and correlation estimation method.
- **e_v2_00:** This script computes dummy variables when marijuana was voted legal and when possession was legal for each state in the US. This script creates several dummy variables reflecting different coding schemes reflecting the month that marijuana was voted legal and when possession itself was legal. Coding can be month or quarterwise.
- **e_v2_01:** This script computes the influence of legalizing marijuana on sentmiment/emotion-related words per state. This script estimates linear-mixed models for sentiment/emotions by an event-variable while including random intercepts for states and accounting for general time trend.The script computes a summary table for the standardized linear-mixed models, a barplot of the predicted unstandardized parameter estimates by event-variable, and the z-standardized time trend of states which legalized marijuana ¬±12 months before and after the event variable. Furthermore the script generates a .png combining the summary table, barplot, and timetrend for both sentiments.
- **e_v3:** This script looks at how activity on r/SandersForPresident changed around state primary election days in 2016 and 2020. It tracks both the number of unique users and the number of posts, asking whether participation spiked on primary days compared to the days before and after. The script produces summary tables and clear visualizations‚Äîsuch as forest plots showing relative activity levels and event-aligned timelines standardized to a common baseline‚Äîthat highlight the sharp surges on primary days and the quick declines afterward. These results demonstrate how online activity mirrors real-world political events.
      
## üõ†Ô∏è 1) Overview of scripts to create GeoLoctated corpus (indicated by c_)

| Step | Dataset Input | Dataset Output | Script Name | Description |
|------|---------------|----------------|-------------|-------------|
| c_00 | RS_YYYY-MM.zst, RC_YYYY-MM.zst | o_RS_YYYY-MM.zst, o_RC_YYYY-MM.zst | c_00_dr_o_Reddit.py | This script guides you to download the original Reddit dataset using qBittorrent and renames the dataset .zst files by adding an o_ prefix to filenames matching RS_YYYY-MM.zst or RC_YYYY-MM.zst. |
| c_01 | o_RS_YYYY-MM.zst, o_RC_YYYY-MM.zst, o_state_info.csv | p_YYYY-MM.csv | c_01_locate_users.py | This script processes monthly Reddit .zst files to count the number of posts made by Reddit users in 2,584 US state-related subreddits (as listed in 'o_state_info.csv' crawled from r/LocationReddits). |
| c_02 | o_RS_YYYY-MM.zst, o_RC_YYYY-MM.zst, o_2005-06to2023-12_unique_authors.csv | p_RS_YYYY-MM.zst, p_RC_YYYY-MM.zst | c_02_extract_geolocated_users.py | This script filters the original Reddit dataset to extract only records belonging to a specified list of geolocated users by using multiprocessing. ‚Äòo_2005-06to2023-12_unique_authors.csv‚Äô is a CSV file listing target usernames and formed by the unique user of 'p_YYYY-MM.csv' files. |
| c_03 | p_RS_YYYY-MM.zst, p_RC_YYYY-MM.zst | p_postnum_YYYY-MM.csv | c_03_post_num.py | This script processes monthly GeoReddit files to count the total number of posts made by each user per month by using multiprocessing. |
| c_04 | p_YYYY-MM.csv + p_postnum_YYYY-MM.csv + o_2005-06to2023-12_ratios.csv | p_YYYY-MMtoYYYY-MM_ratios_postnum.csv | c_04_period_ratio_postnum.py | This script computes ratio1 and ratio2 metrics over a specified period. |
| c_05 | p_YYYY-MMtoYYYY-MM_ratios_postnum.csv | p_YYYY-MMtoYYYY-MM_cumulative_ratios_User/Post.png | c_05_cumulative_ratios.py | This script generates cumulative distribution plots for ratio1 and ratio2, showing how the cumulative proportion of users or posts changes with increasing ratio thresholds (Ratios were log-transformed). |
| c_06 | p_YYYY-MMtoYYYY-MM_ratios_postnum.csv | p_YYYY-MMtoYYYY-MM_ratio2_distribution.png | c_06_log_ratio2_distribution.py | This script visualizes the distribution of log-transformed ratio2 values for GeoReddit users or posts over a specified period. |
| c_07 | p_state_counts_cumulative.csv + o_census.csv | p_YYYY-MMtoYYYY-MM_US_State_Maps.png | c_07_US_state_map.py | This script generates three choropleth maps (US population, GeoReddit user count, and GeoReddit post count) visualizing U.S. state-level data for a specified time period. |
| c_08 | p_state_counts_cumulative.csv + o_census.csv | p_YYYY-MMtoYYYY-MM_density_Maps.png | c_08_US_state_density_map.py | This script generates a choropleth map visualizing geolocated user density per 1,000 residents across U.S. states for a specified time period and calculates its median. |
| c_09 | o_T-YYYY-MM.csv + p_YYYY-MMtoYYYY-MM_ratios_postnum.csv | p_statistics_YYYY-MMtoYYYY-MM.csv | c_09_statistic_table.py | This script generates summary statistics about Reddit user post activity over a specified period. |
| c_10 | o_2005-06to2023-12_filtered_authors.csv, p_YYYY-01toYYYY-12_ratios_postnum.csv | p_geo_robust.csv | c_10_geo_robust.py | Compare year-specific author‚Üístate assignments (ratio2 > 1) to the global assignment, summarizing overlap, mismatches, and coverage by year. |

## üìà 2) Overview of scripts to analyse GeoLoctated corpus (indicated by a_)

| Step | Dataset Input | Dataset Output | Script Name | Description |
|------|---------------|----------------|-------------|-------------|
| a_00 | o_dictionary.txt | p_dictionary.txt | a_00_build_dictionary.py | This script constructs a regex-based fuzzy keyword matching dictionary by converting input phrases and labeled entities into regular expressions that tolerate flexible spacing, punctuation, and edit-distance-1 variants, including character insertions, deletions, and substitutions. This code ensures accurate keyword extraction by using strict matching for short or ambiguous terms, while allowing edit-distance tolerance for longer, semantically unique words. | 
| a_01a | p_RS_YYYY-MM.zst, p_RC_YYYY-MM.zst, o_2005-06to2023-12_filtered_authors.csv, p_dictionary.txt | `p_{topic}_YYYY-MM.csv`, `p_{topic}_total_post.csv`, `p_{topic}_total_author.csv`, `p_{topic}_author_month.csv`, `p_{topic}_post_month.csv`, `p_{topic}_keyword_month.csv` | a_01a_extract_topic_post.py | This script processes geolocated Reddit `.zst` data files (like removing none-English ones etc.) to extract posts related to specific topics (e.g., AI) and to generate statistical summariesÔºåand additionally track peak memory usage and runtime performance per month, using a dictionary of regex patterns that include common misspellings. It can only be run on Mac because of using Hypersan. |
| a_01b | p_RS_YYYY-MM.zst, p_RC_YYYY-MM.zst, o_2005-06to2023-12_filtered_authors.csv, p_dictionary.txt | `p_{topic}_YYYY-MM.csv`, `p_{topic}_total_post.csv`, `p_{topic}_total_author.csv`, `p_{topic}_author_month.csv`, `p_{topic}_post_month.csv` | a_01b_extract_topic_post.py | This script is a faster version of `a_01a`. Unlike `a_01a`, it does NOT count how many times each keyword appears in a post. By default, it extracts only topic-matched posts with basic metadata. If `--full_output` is enabled, it will additionally include list of distinct keywords matched in each post and count of distinct keyword matched. |
| a_01c | p_RS_YYYY-MM.zst, p_RC_YYYY-MM.zst, o_2005-06to2023-12_filtered_authors.csv, p_dictionary.txt, o_subreddits.txt | `p_{topic}_YYYY-MM.csv`, `p_{topic}_total_post.csv`, `p_{topic}_total_author.csv`, `p_{topic}_author_month.csv`, `p_{topic}_post_month.csv`, `p_{topic}_keyword_month.csv` | a_01c_extract_topic_post.py | This script filters Reddit compressed data files by: 1. Keywords (from p_dictionary.txt, regex + label format); 2. Subreddits (from o_subreddits.txt, case-sensitive exact match); 3. Or both (union mode: match if either keywords or subreddits). |
| a_01b | p_RS_YYYY-MM.zst, p_RC_YYYY-MM.zst, o_2005-06to2023-12_filtered_authors.csv, p_dictionary.txt | `p_{topic}_YYYY-MM.csv`, `p_{topic}_total_post.csv`, `p_{topic}_total_author.csv`, `p_{topic}_author_month.csv`, `p_{topic}_post_month.csv` | a_01b_extract_topic_post.py | This script is a faster version of `a_01a`. Unlike `a_01a`, it does NOT count how many times each keyword appears in a post. By default, it extracts only topic-matched posts with basic metadata. If `--full_output` is enabled, it will additionally include list of distinct keywords matched in each post and count of distinct keyword matched. |
| a_03a | p_postnum_YYYY-MM.csv, o_2005-06to2023-12_filtered_authors.csv | p_author_counts_per_month.csv, p_post_counts_per_month.csv, p_author_counts_per_quarter.csv, p_post_counts_per_quarter.csv, p_author_counts_per_year.csv, p_post_counts_per_year.csv | a_03a_gif_map.py | This script aggregates Reddit post counts and author counts by US state and month using geolocated author data (ratio2 > 1). |
| a_03b | `p_{topic}_author_month.csv`, `p_{topic}_post_month.csv`, `p_{data_type}_counts_per_{freq}.csv`, `p_{topic}_YYYY-MM.csv`<br><br>Note:<br>- `{data_type}`: author / post / keyword<br>- `{freq}`: month / quarter / year<br>- `{topic}`: AI etc. | `p_{topic}_{data_type}_{freq}.csv`, `p_{topic}_norm_{data_type}_{freq}.csv`<br><br>Note:<br>- `{data_type}`: author / post / keyword<br>- `{freq}`: month / quarter / year<br>- `{topic}`: AI etc. | a_03b_gif_map.py | Aggregate GeoReddit counts and normalize against total Reddit author/post counts by U.S. state and time window (monthly, quarterly, or yearly) for a given topic. |
| a_03c | `p_{topic}_{data_type}_{freq}.csv`, `p_{topic}_norm_{data_type}_{freq}.csv`<br><br>Note:<br>- `{data_type}`: author / post / keyword<br>- `{freq}`: month / quarter / year<br>- `{topic}`: AI etc. | `p_{topic}_{data_type}_gifmap_{freq}.html` (6), `p_{topic}_{data_type}_gifmap_{freq}_abs.html` (6)<br><br>Note:<br>- `{data_type}`: author / post / keyword<br>- `{freq}`: month / quarter / year<br>- `{topic}`: AI etc. | a_03c_gif_map.py | This script generates an interactive dynamic choropleth map to visualize topic prevalence (user-level or post-level; ratio or absolute count) across US states over time (monthly, quarterly, or yearly aggregation). Specifically, it sets the color bar range using the 95th percentile to prevent extreme monthly values from distorting the color scale, ensuring clearer and more balanced visual comparisons over time.|
| a_04 | `p_{topic}_post.csv`, `p_{topic}_keyword.csv`, `p_{topic}_YYYY-MM.csv` files, p_2005-06to2023-12_state_count.csv, o_2005-06to2023-12_filtered_authors.csv | `p_{topic}_static_info.csv`, `p_{topic}_Static_State_Maps.png`, `p_{topic}_static_info_abs.csv`, `p_{topic}__Static_State_Maps_abs.png` | a_04_static_map_table.py | This script generates state-level statistics (ratio or absolute count) on specific topic prevalence and creates US static choropleth maps visualizing this topic prevalence for the whole period. |
| a_05| `p_{topic}_{data_type}.csv`, `p_{topic}_norm_{data_type}_{freq}.csv` | `p_{topic}_{data_type}_{freq}_{norm/abs}_trend.png` | a_05_line_trend.py | This script generates a line plot visualizing topic trends across U.S. states over time (either monthly, quarterly, or yearly aggregation), using matched Reddit post and user counts or their normalized proportion. For normalized month-level situation, the code first clips extreme values above the 99th percentile to reduce distortion from outliers. Then, it applies a 3-month moving average to smooth short-term fluctuations, making the trend lines more stable and easier to interpret. |
| a_06| o_dictionary.txt, `p_{topic}_YYYY-MM.csv` | p_keyword_counts.csv, p_cooccurrence_edges.csv, p_keyword_totals_summary.csv, p_keyword_degree_summary.csv | a_06_dic_stat.py | This script counts monthly keyword (topic dictionary) occurrences and export co‚Äêoccurrence network and detailed summaries with case-preserving dictionary labels. |
| a_07| p_keyword_counts.csv, p_cooccurrence_edges.csv, p_keyword_totals_summary.csv, p_keyword_degree_summary.csv | p_distribution.png, p_Total Mentions_top_bottom_{n}.png, p_Degree_top_bottom_{n}.png, p_wordcloud_Total Mentions_top_{n}.png, p_wordcloud_Total Mentions_bottom_{n}.png, p_wordcloud_Degree_top_{n}.png, p_wordcloud_Degree_bottom_{n}.png, p_scatter.png, p_trend_top_{n}.png, p_dendrogram.png | a_07_dic_figure.py | This script generates comprehensive dictionary analysis visuals including distributions, top/bottom N words, word clouds, trend lines, co-occurrence scatter, and hierarchical clustering dendrogram. |
| a_08 | p_{topic}_YYYY-MM.csv | `p_{topic}_sentiment_{YYYY-MM}.csv` | a_08_topicspecific_sentiment_NRCL.py | This script identifies emotional words in the post body based on based on NCRLex (National Research Council Emotion Lexicon) . Words are identified to be related to eight emotions: 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust' and two types of sentiment: 'positive', 'negative'. The script counts the number of occurences of words (count), the proportion of how often it appears in comparison to the other emotional words (prop), and the relative share of occurences in comparison to the total amount of words (freq). | 
| a_09 | `p_{topic}_sentiment_{YYYY-MM}.csv` | `{topic}_{outcome_variable}_map_{freq}_{aggregation_method}.html`, `{topic}_combined_sentiment_{mode}_{label}_n{cutoff}.csv`, `{topic}_observation_overview.xlsx` | a_09_sentiment_map_table.py | This script generates interactive dynamic choropleth maps (and their underlying .csv data) to visualize average sentiment (estimated by NRCLex) across US states over time (monthly, quarterly, or yearly aggregation). This script provides different cutoff values >10, >30, >50, >100 observations per state x period combination. Also it gives an overview excel of how many observations there are for each state x period combination. | 
| a_10 | `p_{topic}_sentiment_{YYYY-MM}.csv` | `{topic}_sentiment_timelines_{freq}_{outcome_variable}.png` | a_10_sentiment_linegraph.py | This script generates linegraphs of NRCL emotions and sentiment over time (monthly, quarterly, or yearly aggregation) as .png-files. | 

## 3) Overview of scripts for validation

| Step | Dataset Input | Dataset Output | Script Name | Description |
|------|---------------|----------------|-------------|-------------|
| v_01a | p_postnum_YYYY-MM.csv files, o_2005-06to2023-12_filtered_authors.csv | p_state_counts_cumulative.csv | v_01a_vali_pop.py | Aggregate, for each year from a start date through that year‚Äôs end, the unique Reddit user counts and total post counts by U.S. state, based on geolocated authors, as input for 28 plots in v_01b. |
| v_01b | p_state_counts_cumulative.csv + o_census.csv | p_scatter_plot.png + p_all_years_scatter_plots.png | v_01b_vali_pop_visual.py | This script generates scatter plots showing correlations between U.S. state resident population Census or Estimates (from U.S Census Bureau) and GeoReddit user/post counts (ratio2 >1) over a specified period. |
| v_02a | o_2005-06to2023-12_filtered_authors.csv, p_RS_YYYY-MM.zst, p_RC_YYYY-MM.zst, o_types.csv | p_user_type.csv | v_02a_vali_type.py | This script aggregates geolocated Reddit users‚Äô activity into ‚Äútypes‚Äù (e.g., political orientation, religion) via a subreddit‚Üítype mapping, and outputs per-user post counts by time period (month or day level). |
| v_02b | p_user_type.csv, o_2005-06to2023-12_filtered_authors.csv, p_state_counts_cumulative.csv, o_indicators.csv | p_cor_{TYPE}.csv, p_{TYPE}_{RANK}_scatter.png, p_{TYPE}_{RANK}_map.png/html, p_correlations_overall.csv | v_02b_vali_visual.py | This script computes state-level correlations between subreddit participation (users and posts) and external benchmark indicators (e.g., ACS, BLS). It normalizes subreddit activity by state-level cumulative denominators to ensure comparability, then evaluates correlations for each type and subreddit combination, generating CSV outputs, scatterplots, and optional U.S. maps. |

## 4) Overview of scripts for projects including vignette 1 to 3 (indicated by e_v(Number)_)

| Step | Dataset Input | Dataset Output | Script Name | Description |
|------|---------------|----------------|-------------|-------------|
| e_v1_01 | `p_{topic}_sentiment_{YYYY-MM}.csv` | `Pepsi_combined_timelines.png`, `Pepsi_monthly_unique_authors.csv`, `Pepsi_monthly_mean_NRCL_freq_anger_with_CI.csv` | e_v1_01_pepsi_author_count_and_anger.py | This script generates a .png on the overall count of unique authors posting about Pepsi and their mean NRCL_freq_anger per month with marking the analysed event month of April 2017. Furthermore, the script generates the a .csv file for the respective unique author count and men NRCL_freq_anger data. In addition, the script computes the total amount of authors for the relevant event month, and the relative change in authors commenting, and the relative change in anger-related words overall and per state. | 
| e_v1_02 | `p_{topic}_sentiment_{YYYY-MM}.csv` | `Pepsi_{outcome_variable}_author_event_diff_summary.csv`, `Pepsi_{outcome_variable}_event_diff_map_author_n{threshold}.html`, `Pepsi_{outcome_variable}_event_diff_map_author_n{threshold}.png`, `Pepsi_{outcome_variable}_event_diff_map_author_n{threshold}.jpeg` | e_v1_02_pepsi_event_change_map.py | This script generates choropleth maps (and its corresponding data as .csv) to visualize the change in average sentiment (estimated by NRCLex anger) across US states between April 2017 and the reference period January 2014 - March 2017. Maps are created for NRCL_freq_anger or NRCL_prop_anger and the cutoffs by 30 or 50 observations. | 
| e_v1_03 | `{topic}_{outcome_variable}_author_event_diff_summary.csv`, State_covariates.csv | `{topic}_{outcome_variable}_correlation_forest_and_scatter_n{threshold}_{corr_method}.png` | e_v1_03_pepsi_regional_covariates_forest_plot_with_scatterplot.py | This script generates correlations between sentiment change (i.e, change in anger) and various regional, state-level covariates, and visualizes them in a forest plot. In addition, this script generates a scatterplot for the correlation between the outcome variable and voting Republican in the 2016 presidential election. Plots vary by outcome variable, cutoff, and correlation estimation method. | 
| e_v2_00 | | `monthly_{dummy_coding}.csv`, `quarterly_{dummy_coding}.csv`, `monthly_merged.csv`, `quarterly_merged.csv` | e_v2_00_legalization_dummy_variables.py | This script computes dummy variables when marijuana was voted legal and when possession was legal for each state in the US. This script creates several dummy variables reflecting different coding schemes reflecting the month that marijuana was voted legal and when possession itself was legal. Coding can be month or quarterwise. | 
| e_v2_01 | `Weed_combined_sentiment_per_author_month_n{obs_cutoffs}.csv`, `monthly_merged.csv` | folders of structure `n{obs_cutoffs}_NRCL_{sentiment}`, `summary_grid_{base-rate}_n{obs_cutoffs}_{event_variable}.png` | e_v2_01_legalization_event_analysis.py | This script computes the influence of legalizing marijuana on sentmiment words per state. This script estimates linear-mixed models for sentiment by an event-variable while including random intercepts for states and accounting for general time trend. The script computes a summary table for the standardized linear-mixed models, a barplot of the predicted unstandardized parameter estimates by event-variable, and the z-standardized time. Furthermore the script generates a .png combining the summary table, barplot, and timetrend for both sentiments. | 
| e_v3 | p_user_type.csv, o_2005-06to2023-12_filtered_authors.csv, o_primary_dates.csv | p_summary_users_daily.csv, p_summary_posts_daily.csv, p_forest_daily_7d.png, p_timeseries_daily_90d.png, p_forest_daily_users.png, p_forest_daily_posts.png, p_timeseries_daily_users.png | e_v3_NB-GLMM_visual.R | Analyze daily-level Reddit activity (posts / unique users) around U.S state primary election dates; Construct ¬±N-day windows (N ‚àà {3,7,15,30,60,90}), fit NB-GLMMs (glmmTMB; log link, state random intercept), and extract fixed-effect IRRs (Pre vs Primary, Post vs Primary) with 95% CIs and p-values; Produce summary tables and visualizations (forest plots & event-aligned time series with baseline normalization -90 to -8 days). |


